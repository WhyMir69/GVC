{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE\n",
    "- if mo ask si maam why this is similar to mine and mag suspect siya na i made it, kay just tell her na youre using mediapipe instead of tensorflow\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flaw/weaknesses observed:\n",
    "\n",
    "evident luminance affecting performance, resulting to erratic tracking which causing it to abruptly stop collecting and considering it as one execution\n",
    "- should always perform/collect data at a well lit environment\n",
    "- should apply frame patience mechanism wherein it ignores certain amount of frames before it consider as an execution [APPLIED!]\n",
    "\n",
    "overload of data\n",
    "- apply frame buffering(less frame already is sufficient enough to resemble the movement)\n",
    "- eliminate z coordinate(pose estimation is positional dependant)\n",
    "- experiment on lessening decimal places (this would sacrifice a certain amount of accuracy, should heavily consider how much of it)\n",
    "\n",
    "\n",
    "future planned features\n",
    " - real-time frame by frame visualization every after execution to check\n",
    " - easy-delete button wherein it deletes the current execution collected in case its not good or intended \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How data collection works?\n",
    "1) show your hands on the camera----------------> data collection starts\n",
    "2) perform the action you want -----------------> coordinates are collected(coordinates_frame are generated and are compiled in coordinates_movemenet )\n",
    "3) remove your hand from the camera ------------> data collection stops(coordinates_movemenet is compiled in execution)\n",
    "4) repeat...\n",
    "\n",
    "coordinates\n",
    "- individual X and Y coordinates tracked and collected \n",
    "- how it looks like: 0.435. 0.454....etc\n",
    "\n",
    "\n",
    "coordinates_frame\n",
    "- compilation of coordinates\n",
    "- how it looks like: [coordinates, coordinates, coordinates, coordinates..., coordinates,]\n",
    "\n",
    "execution\n",
    "- compilation of coordinates_frame, which resembles a movement \n",
    "- how it looks like:\n",
    "[\n",
    "[coordinates, coordinates, coordinates, coordinates..., coordinates,],\n",
    "[coordinates, coordinates, coordinates, coordinates..., coordinates,],\n",
    "...\n",
    "[coordinates, coordinates, coordinates, coordinates..., coordinates,],\n",
    "]\n",
    "\n",
    "datasets\n",
    "- compilation of executions\n",
    "- how it looks like:\n",
    "[\n",
    "execution,\n",
    "execution,\n",
    "execution,\n",
    "...\n",
    "execution,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from threading import Thread\n",
    "import os\n",
    "\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Globals\n",
    "detection_mode = \"Multiple\"\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "label_map = {}\n",
    "trained = False\n",
    "\n",
    "# MediaPipe for hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "hands_detector = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "known_folder = os.path.join(os.getcwd(), \"datasets/\")\n",
    "parent_folder = os.path.dirname(known_folder)\n",
    "file_path = os.path.join(parent_folder, \"output.txt\")\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "data_pad = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pose_estimation():\n",
    "    frame_buffer= 8\n",
    "    frame_buffer_ctr = 0\n",
    "\n",
    "    frame_patience = 8\n",
    "    frame_patience_ctr = 0\n",
    "    \n",
    "    coordinates_frame = []\n",
    "    coordinates_movemenet = []\n",
    "    execution = []\n",
    "    initialize_padding()\n",
    "\n",
    "    while True:        \n",
    "        frame_buffer_ctr+=1\n",
    "        if frame_buffer_ctr>=frame_buffer:   \n",
    "            frame_buffer_ctr = 0         \n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands_detector.process(rgb)\n",
    "\n",
    "            if results.multi_hand_landmarks:    \n",
    "                frame_patience_ctr=0\n",
    "\n",
    "            if results.multi_hand_landmarks and results.multi_handedness:\n",
    "\n",
    "                detected_hands = [hand.classification[0].label for hand in results.multi_handedness]\n",
    "\n",
    "                for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    hand_label = hand_handedness.classification[0].label \n",
    "                    print(f\"Detected {hand_label} hand -> {len(coordinates_frame)}\")\n",
    "                    mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "                        coordinates_frame.append(f\"{landmark.x:.3f}\")\n",
    "                        coordinates_frame.append(f\"{landmark.y:.3f}\")\n",
    "\n",
    "                if 'Left' in detected_hands and 'Right' not in detected_hands and len(coordinates_frame) != 84:\n",
    "                    coordinates_frame.extend(data_pad)\n",
    "\n",
    "                elif 'Right' in detected_hands and 'Left'not in detected_hands  and len(coordinates_frame) != 84:\n",
    "                    temp = coordinates_frame\n",
    "                    coordinates_frame = []                    \n",
    "                    coordinates_frame.extend(data_pad)\n",
    "                    coordinates_frame.extend(temp)\n",
    "                     \n",
    "                coordinates_movemenet.append(coordinates_frame)\n",
    "                coordinates_frame=[]\n",
    "            else:\n",
    "                if frame_patience_ctr<=frame_patience:\n",
    "                    frame_patience_ctr += 1\n",
    "\n",
    "                if frame_patience_ctr >= frame_patience:\n",
    "                    if len(coordinates_movemenet) != 0:    \n",
    "                        execution.append(coordinates_movemenet)\n",
    "                        print(\"execution collected -> \", len(execution),\"[\",len(coordinates_movemenet) ,\"]\")\n",
    "\n",
    "                        coordinates_movemenet = []\n",
    "\n",
    "            cv2.imshow(\"Recording Face and Hand\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()  \n",
    "    return execution\n",
    "\n",
    "\n",
    "def inferencing():\n",
    "    frame_buffer= 8\n",
    "    frame_buffer_ctr = 0\n",
    "\n",
    "    frame_patience = 8\n",
    "    frame_patience_ctr = 0\n",
    "    \n",
    "    coordinates_frame = []\n",
    "    coordinates_movemenet = []\n",
    "    execution = []\n",
    "    initialize_padding()\n",
    "\n",
    "    while True:        \n",
    "        frame_buffer_ctr+=1\n",
    "        if frame_buffer_ctr>=frame_buffer:   \n",
    "            frame_buffer_ctr = 0         \n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands_detector.process(rgb)\n",
    "\n",
    "            if results.multi_hand_landmarks:    \n",
    "                frame_patience_ctr=0\n",
    "\n",
    "            if results.multi_hand_landmarks and results.multi_handedness:\n",
    "\n",
    "                detected_hands = [hand.classification[0].label for hand in results.multi_handedness]\n",
    "\n",
    "                for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    hand_label = hand_handedness.classification[0].label \n",
    "                    print(f\"Detected {hand_label} hand -> {len(coordinates_frame)}\")\n",
    "                    mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "                        coordinates_frame.append(f\"{landmark.x:.3f}\")\n",
    "                        coordinates_frame.append(f\"{landmark.y:.3f}\")\n",
    "\n",
    "                if 'Left' in detected_hands and 'Right' not in detected_hands and len(coordinates_frame) != 84:\n",
    "                    coordinates_frame.extend(data_pad)\n",
    "\n",
    "                elif 'Right' in detected_hands and 'Left'not in detected_hands  and len(coordinates_frame) != 84:\n",
    "                    temp = coordinates_frame\n",
    "                    coordinates_frame = []                    \n",
    "                    coordinates_frame.extend(data_pad)\n",
    "                    coordinates_frame.extend(temp)\n",
    "                     \n",
    "                coordinates_movemenet.append(coordinates_frame)\n",
    "                coordinates_frame=[]\n",
    "            else:\n",
    "                if frame_patience_ctr<=frame_patience:\n",
    "                    frame_patience_ctr += 1\n",
    "\n",
    "                if frame_patience_ctr >= frame_patience:\n",
    "                    if len(coordinates_movemenet) != 0:    \n",
    "                        execution.append(coordinates_movemenet)\n",
    "                        print(\"execution collected -> \", len(execution),\"[\",len(coordinates_movemenet) ,\"]\")\n",
    "\n",
    "                        coordinates_movemenet = []\n",
    "\n",
    "            cv2.imshow(\"Recording Face and Hand\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()  \n",
    "    return execution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text_encode(coordinates_data):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for execution in coordinates_data:\n",
    "            file.write(\"START\\n\")\n",
    "            for movement_coordinates in execution:\n",
    "                for coordinate in movement_coordinates:\n",
    "                    file.write(str(coordinate))\n",
    "                    file.write(\"|\")\n",
    "                file.write(\"\\n\")\n",
    "            file.write(\"END\\n\")\n",
    "\n",
    "\n",
    "def text_decode(file_path):\n",
    "    coordinates_data = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    execution = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == \"START\":\n",
    "            execution = []\n",
    "        elif line == \"END\":\n",
    "            coordinates_data.append(execution)\n",
    "        else:\n",
    "            str_coords = line.split(\"|\")[:-1]\n",
    "            float_coords = [float(coord) for coord in str_coords]\n",
    "            execution.append(float_coords)\n",
    "\n",
    "    return coordinates_data\n",
    "\n",
    "\n",
    "\n",
    "def initialize_dummy_dataset():\n",
    "    frame_coordinate = []\n",
    "    movement_coordinate = []\n",
    "    executions = []\n",
    "\n",
    "    for x in range(60):\n",
    "        frame_coordinate.append(0.69)\n",
    "\n",
    "    for i in range(random.randint(8, 15)):    \n",
    "        for x in range(random.randint(8, 15)):\n",
    "            movement_coordinate.append(frame_coordinate)\n",
    "        executions.append(movement_coordinate)\n",
    "        movement_coordinate = []\n",
    "\n",
    "    return executions\n",
    "\n",
    "def initialize_padding():\n",
    "    if len(data_pad) != 42:\n",
    "        for ctr in range(42):\n",
    "            data_pad.append(0.0);\n",
    "        print(f\"len of data pad {len(data_pad)}\")\n",
    "\n",
    "\n",
    "\n",
    "def decode_all_txt_files(folder_path):\n",
    "    all_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data = text_decode(file_path)\n",
    "            all_data.append(data) \n",
    "\n",
    "    return all_data\n",
    "\n",
    "def data_padding(input_datasets):\n",
    "    initialize_padding()\n",
    "    datasets = input_datasets\n",
    "    max_len = 0\n",
    "    padding = []\n",
    "    padding.extend(data_pad)\n",
    "    padding.extend(data_pad)\n",
    "    print(len(data_pad))\n",
    "\n",
    "\n",
    "    for classifications in datasets:\n",
    "        for executions in classifications:\n",
    "            if max_len <= len(executions):\n",
    "                max_len = len(executions)\n",
    "\n",
    "    for classification in datasets:\n",
    "        for executions in classification:\n",
    "            while(len(executions) < max_len):\n",
    "                executions.append(padding)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def data_labelling(input_dataset):\n",
    "    temp_input_dataset = input_dataset\n",
    "    label = []\n",
    "    data = []\n",
    "    for ctr in range(len(temp_input_dataset)):\n",
    "        for ctr2 in range(len(temp_input_dataset[ctr])):\n",
    "            label.append(ctr)\n",
    "            data.append(temp_input_dataset[ctr][ctr2])\n",
    "    return [data,label]\n",
    "\n",
    "\n",
    "\n",
    "# def coordinates_visualization(coordintes_dataset):\n",
    "#     canvas_size = 500\n",
    "#     canvas = np.zeros((canvas_size, canvas_size * 2, 3), dtype=np.uint8)\n",
    "\n",
    "#     # for execution in coordintes_dataset:\n",
    "#         # for frame_coordinates in execution[0]:\n",
    "#         #     # for individual_coordinates in frame_coordinates:\n",
    "#         #     for individual_coordinates in range(len(frame_coordinates)):\n",
    "#         #         if individual_coordinates + 1 < len(frame_coordinates):\n",
    "#         #             if individual_coordinates % 2 != 0:\n",
    "#         #                 cv2.circle(canvas, (int(frame_coordinates[individual_coordinates]*1000),int(frame_coordinates[individual_coordinates+1]*1000)), radius=4, color=(0, 255, 0), thickness=-1)\n",
    "#     print(len(coordintes_dataset[0]))\n",
    "#     for frame_coordinates in coordintes_dataset[0]:\n",
    "#         # for individual_coordinates in frame_coordinates:\n",
    "#         for individual_coordinates in range(len(frame_coordinates)):\n",
    "#             if individual_coordinates + 1 < len(frame_coordinates):\n",
    "#                 if individual_coordinates % 2 != 0:\n",
    "#                     cv2.circle(canvas, (int(frame_coordinates[individual_coordinates]*1000),int(frame_coordinates[individual_coordinates+1]*1000)), radius=4, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "#     cv2.imshow(\"Painted Coordinates\", canvas)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "#     return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting dataset\n",
    "text_encode(pose_estimation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "temp = decode_all_txt_files(parent_folder)\n",
    "temp_padded = data_padding(temp)\n",
    "label = data_labelling(temp_padded)\n",
    "data_shuffled, label_shuffled = shuffle(label[0], label[1], random_state=42)\n",
    "data_shuffled = np.array(data_shuffled)\n",
    "label_shuffled = np.array(label_shuffled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "13/13 [==============================] - 7s 155ms/step - loss: 0.6893 - accuracy: 0.5773 - val_loss: 0.6631 - val_accuracy: 0.5600\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.6442 - accuracy: 0.6289 - val_loss: 0.9001 - val_accuracy: 0.6400\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.6400 - accuracy: 0.6598 - val_loss: 0.6371 - val_accuracy: 0.6400\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.5510 - accuracy: 0.7835 - val_loss: 0.4984 - val_accuracy: 0.8400\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.3988 - accuracy: 0.7938 - val_loss: 0.3800 - val_accuracy: 0.8800\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.3224 - accuracy: 0.9072 - val_loss: 0.2659 - val_accuracy: 0.9200\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.3833 - accuracy: 0.8660 - val_loss: 0.3597 - val_accuracy: 0.8800\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.2484 - accuracy: 0.9175 - val_loss: 0.3499 - val_accuracy: 0.8000\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.2585 - accuracy: 0.8969 - val_loss: 0.3418 - val_accuracy: 0.8800\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.1941 - accuracy: 0.9278 - val_loss: 0.2041 - val_accuracy: 0.9200\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.2205 - accuracy: 0.9175 - val_loss: 0.3188 - val_accuracy: 0.8000\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1755 - accuracy: 0.9381 - val_loss: 0.2902 - val_accuracy: 0.8000\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.2375 - accuracy: 0.9278 - val_loss: 0.4208 - val_accuracy: 0.8400\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.7109 - accuracy: 0.6804 - val_loss: 0.4910 - val_accuracy: 0.7600\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.4536 - accuracy: 0.7423 - val_loss: 0.4201 - val_accuracy: 0.7600\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.3989 - accuracy: 0.8247 - val_loss: 0.3734 - val_accuracy: 0.8000\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.3451 - accuracy: 0.8660 - val_loss: 0.3580 - val_accuracy: 0.8800\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.2957 - accuracy: 0.9072 - val_loss: 0.3000 - val_accuracy: 0.8800\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.2336 - accuracy: 0.9381 - val_loss: 0.3373 - val_accuracy: 0.8800\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.2726 - accuracy: 0.9072 - val_loss: 0.3186 - val_accuracy: 0.8400\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.2046 - accuracy: 0.9278 - val_loss: 0.3636 - val_accuracy: 0.8800\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.2506 - accuracy: 0.8969 - val_loss: 0.2943 - val_accuracy: 0.8400\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.1938 - accuracy: 0.9278 - val_loss: 0.3648 - val_accuracy: 0.8400\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.1999 - accuracy: 0.9278 - val_loss: 0.2696 - val_accuracy: 0.8800\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.2536 - accuracy: 0.9072 - val_loss: 0.2880 - val_accuracy: 0.9200\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.2809 - accuracy: 0.9072 - val_loss: 0.2678 - val_accuracy: 0.8400\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.2738 - accuracy: 0.8969 - val_loss: 0.3653 - val_accuracy: 0.8400\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.2008 - accuracy: 0.9175 - val_loss: 0.2623 - val_accuracy: 0.8800\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.1805 - accuracy: 0.9278 - val_loss: 0.2439 - val_accuracy: 0.8400\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.1646 - accuracy: 0.9381 - val_loss: 0.2614 - val_accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "num_classification = 2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_shuffled, label_shuffled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(data_shuffled.shape[1], data_shuffled.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classification, activation='softmax'))  # num_classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=8, validation_data=(X_test, y_test))\n",
    "model.save(\"my_model.h5\")  # or use .keras for newer format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
